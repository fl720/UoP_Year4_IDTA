{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24169dd5",
   "metadata": {},
   "source": [
    "# IMDB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a91bd",
   "metadata": {},
   "source": [
    "## -------------------------- Load UP --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382f203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import os  \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')            \n",
    "nltk.download('punkt_tab')        \n",
    "nltk.download('omw-1.4')                 \n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b8a86d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='\\t', header=None, names=['review', 'sentiment'])\n",
    "print(\"Dataset loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ece64ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (748, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf630fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows:\n",
      "                                                                                                                                                                                         review  \\\n",
      "0                                                                                                       A very, very, very slow-moving, aimless movie about a distressed, drifting young man.     \n",
      "1                                                                                           Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.     \n",
      "2  Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.     \n",
      "3                                                                                                                                                  Very little music or anything to speak of.     \n",
      "4                                                                                  The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.     \n",
      "\n",
      "   sentiment  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          1  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff175a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "Number of reviews: 748\n",
      "Number of positive reviews (1): 386\n",
      "Number of negative reviews (0): 362\n",
      "\n",
      "Sample reviews:\n",
      "\n",
      "Review 1 (Sentiment: 0):\n",
      "  A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  ...\n",
      "\n",
      "Review 2 (Sentiment: 0):\n",
      "  Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  ...\n",
      "\n",
      "Review 3 (Sentiment: 0):\n",
      "  Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the...\n"
     ]
    }
   ],
   "source": [
    "# data info\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Number of reviews: {len(df)}\")\n",
    "print(f\"Number of positive reviews (1): {sum(df['sentiment'] == 1)}\")\n",
    "print(f\"Number of negative reviews (0): {sum(df['sentiment'] == 0)}\")\n",
    "print(\"\\nSample reviews:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nReview {i+1} (Sentiment: {df['sentiment'].iloc[i]}):\")\n",
    "    print(f\"  {df['review'].iloc[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2834e095",
   "metadata": {},
   "source": [
    "## Question 1 : Preprocess\n",
    "1) removing punctuation, \n",
    "2) removing numbers, \n",
    "3) removing stop words, \n",
    "4) changing the text to lower/upper case  \n",
    "5) lemmatising.\n",
    "\n",
    "Describe in detail, with at least 3 examples,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "550428c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, steps_to_apply=None):\n",
    "    \"\"\"\n",
    "    Apply various preprocessing steps to text.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The input text to preprocess\n",
    "    steps_to_apply : list or None\n",
    "        List of preprocessing steps to apply. If None, apply all steps.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing original text and results of each preprocessing step\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'original': text,\n",
    "        'lowercase': None,\n",
    "        'no_punctuation': None,\n",
    "        'no_numbers': None,\n",
    "        'no_stopwords': None,\n",
    "        'lemmatized': None,\n",
    "        'final_processed': None\n",
    "    }\n",
    "    \n",
    "    # Define which steps to apply\n",
    "    if steps_to_apply is None:\n",
    "        steps_to_apply = ['lowercase', 'no_punctuation', 'no_numbers', 'no_stopwords', 'lemmatized']\n",
    "    \n",
    "    # 1: Convert to lowercase\n",
    "    if 'lowercase' in steps_to_apply:\n",
    "        text_lower = text.lower()\n",
    "        results['lowercase'] = text_lower\n",
    "    else:\n",
    "        text_lower = text\n",
    "        results['lowercase'] = text\n",
    "    \n",
    "    # 2: Remove punctuation\n",
    "    if 'no_punctuation' in steps_to_apply:\n",
    "        # Create a translation table to remove punctuation\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text_no_punct = text_lower.translate(translator)\n",
    "        results['no_punctuation'] = text_no_punct\n",
    "    else:\n",
    "        text_no_punct = text_lower\n",
    "        results['no_punctuation'] = text_lower\n",
    "    \n",
    "    # 3: Remove numbers\n",
    "    if 'no_numbers' in steps_to_apply:\n",
    "        # Remove digits\n",
    "        text_no_numbers = re.sub(r'\\d+', '', text_no_punct)\n",
    "        results['no_numbers'] = text_no_numbers\n",
    "    else:\n",
    "        text_no_numbers = text_no_punct\n",
    "        results['no_numbers'] = text_no_punct\n",
    "    \n",
    "    # 4: Remove stopwords\n",
    "    if 'no_stopwords' in steps_to_apply:\n",
    "        # Get English stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text_no_numbers)\n",
    "        # Remove stopwords\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "        # Reconstruct the text\n",
    "        text_no_stopwords = ' '.join(filtered_tokens)\n",
    "        results['no_stopwords'] = text_no_stopwords\n",
    "    else:\n",
    "        text_no_stopwords = text_no_numbers\n",
    "        results['no_stopwords'] = text_no_numbers\n",
    "    \n",
    "    # 5: Lemmatization\n",
    "    if 'lemmatized' in steps_to_apply:\n",
    "        # Initialize lemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Function to get POS tag for lemmatization\n",
    "        def get_wordnet_pos(treebank_tag):\n",
    "            if treebank_tag.startswith('J'):\n",
    "                return 'a'  # adjective\n",
    "            elif treebank_tag.startswith('V'):\n",
    "                return 'v'  # verb\n",
    "            elif treebank_tag.startswith('N'):\n",
    "                return 'n'  # noun\n",
    "            elif treebank_tag.startswith('R'):\n",
    "                return 'r'  # adverb\n",
    "            else:\n",
    "                return 'n'  # default to noun\n",
    "        \n",
    "        # Tokenize text\n",
    "        tokens = word_tokenize(text_no_stopwords)\n",
    "        # Get POS tags\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        # Lemmatize each word with appropriate POS tag\n",
    "        lemmatized_tokens = []\n",
    "        for word, tag in pos_tags:\n",
    "            pos = get_wordnet_pos(tag)\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, pos)\n",
    "            lemmatized_tokens.append(lemmatized_word)\n",
    "        \n",
    "        # Reconstruct the text\n",
    "        text_lemmatized = ' '.join(lemmatized_tokens)\n",
    "        results['lemmatized'] = text_lemmatized\n",
    "    else:\n",
    "        text_lemmatized = text_no_stopwords\n",
    "        results['lemmatized'] = text_no_stopwords\n",
    "    \n",
    "    # Final processed text\n",
    "    results['final_processed'] = text_lemmatized\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b3d7d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected examples for preprocessing demonstration:\n",
      "================================================================================\n",
      "\n",
      "Example 1 (Index: 0, Sentiment: 0):\n",
      "  Original: A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \n",
      "\n",
      "Example 2 (Index: 10, Sentiment: 1):\n",
      "  Original: And those baby owls were adorable.  \n",
      "\n",
      "Example 3 (Index: 50, Sentiment: 0):\n",
      "  Original: The directing and the cinematography aren't quite as good.  \n"
     ]
    }
   ],
   "source": [
    "# Select 3 examples \n",
    "example_indices = [0, 10, 50]  # You can adjust these indices based on your dataset\n",
    "\n",
    "print(\"Selected examples for preprocessing demonstration:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "examples = []\n",
    "for idx in example_indices:\n",
    "    if idx < len(df):\n",
    "        example = {\n",
    "            'index': idx,\n",
    "            'original_text': df['review'].iloc[idx],\n",
    "            'sentiment': df['sentiment'].iloc[idx]\n",
    "        }\n",
    "        examples.append(example)\n",
    "        print(f\"\\nExample {len(examples)} (Index: {idx}, Sentiment: {example['sentiment']}):\")\n",
    "        print(f\"  Original: {example['original_text']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d6fd97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPROCESSING STEP-BY-STEP DEMONSTRATION\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "EXAMPLE 1 PROCESSING\n",
      "========================================\n",
      "Original text (Sentiment: 0):\n",
      "  \"A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \"\n",
      "\n",
      "1. Lowercase Conversion:\n",
      "  \"a very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \"\n",
      "\n",
      "2. Remove Punctuation:\n",
      "  \"a very very very slowmoving aimless movie about a distressed drifting young man  \"\n",
      "\n",
      "3. Remove Numbers:\n",
      "  \"a very very very slowmoving aimless movie about a distressed drifting young man  \"\n",
      "\n",
      "4. Remove Stopwords:\n",
      "  \"slowmoving aimless movie distressed drifting young man\"\n",
      "  Removed stopwords: about, a, very\n",
      "\n",
      "5. Lemmatization:\n",
      "  \"slowmoving aimless movie distress drift young man\"\n",
      "  Examples of lemmatization:\n",
      "    'distressed' → 'distress'\n",
      "    'drifting' → 'drift'\n",
      "\n",
      "========================================\n",
      "EXAMPLE 2 PROCESSING\n",
      "========================================\n",
      "Original text (Sentiment: 1):\n",
      "  \"And those baby owls were adorable.  \"\n",
      "\n",
      "1. Lowercase Conversion:\n",
      "  \"and those baby owls were adorable.  \"\n",
      "\n",
      "2. Remove Punctuation:\n",
      "  \"and those baby owls were adorable  \"\n",
      "\n",
      "3. Remove Numbers:\n",
      "  \"and those baby owls were adorable  \"\n",
      "\n",
      "4. Remove Stopwords:\n",
      "  \"baby owls adorable\"\n",
      "  Removed stopwords: and, those, were\n",
      "\n",
      "5. Lemmatization:\n",
      "  \"baby owls adorable\"\n",
      "\n",
      "========================================\n",
      "EXAMPLE 3 PROCESSING\n",
      "========================================\n",
      "Original text (Sentiment: 0):\n",
      "  \"The directing and the cinematography aren't quite as good.  \"\n",
      "\n",
      "1. Lowercase Conversion:\n",
      "  \"the directing and the cinematography aren't quite as good.  \"\n",
      "\n",
      "2. Remove Punctuation:\n",
      "  \"the directing and the cinematography arent quite as good  \"\n",
      "\n",
      "3. Remove Numbers:\n",
      "  \"the directing and the cinematography arent quite as good  \"\n",
      "\n",
      "4. Remove Stopwords:\n",
      "  \"directing cinematography arent quite good\"\n",
      "  Removed stopwords: as, and, the\n",
      "\n",
      "5. Lemmatization:\n",
      "  \"direct cinematography arent quite good\"\n",
      "  Examples of lemmatization:\n",
      "    'directing' → 'direct'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING STEP-BY-STEP DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process each example through each step\n",
    "for i, example in enumerate(examples):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"EXAMPLE {i+1} PROCESSING\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"Original text (Sentiment: {example['sentiment']}):\")\n",
    "    print(f\"  \\\"{example['original_text']}\\\"\")\n",
    "    \n",
    "    # Apply each preprocessing step sequentially\n",
    "    steps = [\n",
    "        ('1. Lowercase Conversion', ['lowercase']),\n",
    "        ('2. Remove Punctuation', ['lowercase', 'no_punctuation']),\n",
    "        ('3. Remove Numbers', ['lowercase', 'no_punctuation', 'no_numbers']),\n",
    "        ('4. Remove Stopwords', ['lowercase', 'no_punctuation', 'no_numbers', 'no_stopwords']),\n",
    "        ('5. Lemmatization', ['lowercase', 'no_punctuation', 'no_numbers', 'no_stopwords', 'lemmatized'])\n",
    "    ]\n",
    "    \n",
    "    for step_name, steps_to_apply in steps:\n",
    "        result = preprocess_text(example['original_text'], steps_to_apply)\n",
    "        print(f\"\\n{step_name}:\")\n",
    "        print(f\"  \\\"{result['lemmatized']}\\\"\")\n",
    "        \n",
    "        # Show specific changes for certain steps\n",
    "        if step_name == '4. Remove Stopwords':\n",
    "            # Show removed stopwords\n",
    "            original_tokens = word_tokenize(result['no_numbers'])\n",
    "            filtered_tokens = word_tokenize(result['no_stopwords'])\n",
    "            removed_words = set(original_tokens) - set(filtered_tokens)\n",
    "            stopword_set = set(stopwords.words('english'))\n",
    "            actual_stopwords_removed = [w for w in removed_words if w in stopword_set]\n",
    "            if actual_stopwords_removed:\n",
    "                print(f\"  Removed stopwords: {', '.join(actual_stopwords_removed)}\")\n",
    "        \n",
    "        elif step_name == '5. Lemmatization':\n",
    "            # Show examples of lemmatization\n",
    "            before_tokens = word_tokenize(result['no_stopwords'])\n",
    "            after_tokens = word_tokenize(result['lemmatized'])\n",
    "            \n",
    "            # Find words that changed\n",
    "            changed_words = []\n",
    "            for before, after in zip(before_tokens, after_tokens):\n",
    "                if before != after:\n",
    "                    changed_words.append((before, after))\n",
    "            \n",
    "            if changed_words:\n",
    "                print(f\"  Examples of lemmatization:\")\n",
    "                for before, after in changed_words[:5]:  # Show first 5 changes\n",
    "                    print(f\"    '{before}' → '{after}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c46c9d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: ORIGINAL vs FULLY PROCESSED TEXT\n",
      "================================================================================\n",
      "\n",
      "Example 1 (Sentiment: 0):\n",
      "\n",
      "Original text:\n",
      "  A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \n",
      "\n",
      "Fully processed text (after all 5 steps):\n",
      "  slowmoving aimless movie distress drift young man\n",
      "\n",
      "Statistics:\n",
      "  Original word count: 18\n",
      "  Processed word count: 7\n",
      "  Words removed: 11 (61.1% reduction)\n",
      "\n",
      "Example 2 (Sentiment: 1):\n",
      "\n",
      "Original text:\n",
      "  And those baby owls were adorable.  \n",
      "\n",
      "Fully processed text (after all 5 steps):\n",
      "  baby owls adorable\n",
      "\n",
      "Statistics:\n",
      "  Original word count: 7\n",
      "  Processed word count: 3\n",
      "  Words removed: 4 (57.1% reduction)\n",
      "\n",
      "Example 3 (Sentiment: 0):\n",
      "\n",
      "Original text:\n",
      "  The directing and the cinematography aren't quite as good.  \n",
      "\n",
      "Fully processed text (after all 5 steps):\n",
      "  direct cinematography arent quite good\n",
      "\n",
      "Statistics:\n",
      "  Original word count: 11\n",
      "  Processed word count: 5\n",
      "  Words removed: 6 (54.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "#  Original vs Fully Processed Text\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: ORIGINAL vs FULLY PROCESSED TEXT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, example in enumerate(examples):\n",
    "    # Get fully processed text\n",
    "    full_result = preprocess_text(example['original_text'])\n",
    "    \n",
    "    print(f\"\\nExample {i+1} (Sentiment: {example['sentiment']}):\")\n",
    "    print(f\"\\nOriginal text:\")\n",
    "    print(f\"  {example['original_text']}\")\n",
    "    \n",
    "    print(f\"\\nFully processed text (after all 5 steps):\")\n",
    "    print(f\"  {full_result['final_processed']}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    original_words = len(word_tokenize(example['original_text']))\n",
    "    processed_words = len(word_tokenize(full_result['final_processed']))\n",
    "    reduction = original_words - processed_words\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Original word count: {original_words}\")\n",
    "    print(f\"  Processed word count: {processed_words}\")\n",
    "    print(f\"  Words removed: {reduction} ({reduction/original_words*100:.1f}% reduction)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07dc0cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLYING PREPROCESSING TO ENTIRE DATASET\n",
      "================================================================================\n",
      "Applying preprocessing to all reviews...\n",
      "\n",
      "Sample comparisons (first 5 rows):\n",
      "----------------------------------------\n",
      "\n",
      "Review 0 (Sentiment: 0):\n",
      "  Original: A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  ...\n",
      "  Processed: slowmoving aimless movie distress drift young man...\n",
      "\n",
      "Review 1 (Sentiment: 0):\n",
      "  Original: Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  ...\n",
      "  Processed: sure lose flat character audience nearly half walk...\n",
      "\n",
      "Review 2 (Sentiment: 0):\n",
      "  Original: Attempting artiness with black & white and clever camera angles, the movie disappointed - became eve...\n",
      "  Processed: attempt artiness black white clever camera angle movie disappoint become even ridiculous act poor pl...\n",
      "\n",
      "Review 3 (Sentiment: 0):\n",
      "  Original: Very little music or anything to speak of.  ...\n",
      "  Processed: little music anything speak...\n",
      "\n",
      "Review 4 (Sentiment: 1):\n",
      "  Original: The best scene in the movie was when Gerardo is trying to find a song that keeps running through his...\n",
      "  Processed: best scene movie gerardo try find song keep run head...\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS OF PREPROCESSING IMPACT\n",
      "================================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total reviews: 748\n",
      "  Average original length: 22.59 words\n",
      "  Average processed length: 9.98 words\n",
      "  Average reduction: 12.61 words\n",
      "  Percentage reduction: 55.81%\n",
      "\n",
      "Most common words BEFORE preprocessing:\n",
      "  '.': 905 occurrences\n",
      "  'the': 845 occurrences\n",
      "  ',': 649 occurrences\n",
      "  'and': 433 occurrences\n",
      "  'a': 432 occurrences\n",
      "  'of': 375 occurrences\n",
      "  'is': 343 occurrences\n",
      "  'it': 322 occurrences\n",
      "  'i': 298 occurrences\n",
      "  'this': 292 occurrences\n",
      "\n",
      "Most common words AFTER preprocessing:\n",
      "  'movie': 207 occurrences\n",
      "  'film': 183 occurrences\n",
      "  'bad': 84 occurrences\n",
      "  'one': 79 occurrences\n",
      "  'see': 66 occurrences\n",
      "  'good': 64 occurrences\n",
      "  'make': 60 occurrences\n",
      "  'character': 58 occurrences\n",
      "  'like': 56 occurrences\n",
      "  'time': 47 occurrences\n",
      "\n",
      "Processed dataset saved as 'imdb_reviews_processed.csv'\n",
      "\n",
      "Summary table for report (first 100 characters shown):\n",
      "   Example Sentiment                                                                            Original_Text                                                                                Lowercase                                                                     No_Punctuation                                                                         No_Numbers                                            No_Stopwords                                         Lemmatized\n",
      "0        1  Negative  A very, very, very slow-moving, aimless movie about a distressed, drifting young man.    a very, very, very slow-moving, aimless movie about a distressed, drifting young man.    a very very very slowmoving aimless movie about a distressed drifting young man    a very very very slowmoving aimless movie about a distressed drifting young man    slowmoving aimless movie distressed drifting young man  slowmoving aimless movie distress drift young man\n",
      "1        2  Positive                                                     And those baby owls were adorable.                                                       and those baby owls were adorable.                                                  and those baby owls were adorable                                                  and those baby owls were adorable                                        baby owls adorable                                 baby owls adorable\n",
      "2        3  Negative                             The directing and the cinematography aren't quite as good.                               the directing and the cinematography aren't quite as good.                           the directing and the cinematography arent quite as good                           the directing and the cinematography arent quite as good                 directing cinematography arent quite good             direct cinematography arent quite good\n",
      "\n",
      "Preprocessing examples summary saved as 'preprocessing_examples_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## 1.7 Apply Preprocessing to Entire Dataset\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"APPLYING PREPROCESSING TO ENTIRE DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Apply preprocessing to all reviews\n",
    "print(\"Applying preprocessing to all reviews...\")\n",
    "df['processed_review'] = df['review'].apply(lambda x: preprocess_text(x)['final_processed'])\n",
    "\n",
    "# Show before/after comparison for a few samples\n",
    "print(\"\\nSample comparisons (first 5 rows):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "sample_df = df.head().copy()\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"\\nReview {idx} (Sentiment: {row['sentiment']}):\")\n",
    "    print(f\"  Original: {row['review'][:100]}...\")\n",
    "    print(f\"  Processed: {row['processed_review'][:100]}...\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS OF PREPROCESSING IMPACT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate statistics for the entire dataset\n",
    "original_lengths = df['review'].apply(lambda x: len(word_tokenize(x)))\n",
    "processed_lengths = df['processed_review'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total reviews: {len(df)}\")\n",
    "print(f\"  Average original length: {original_lengths.mean():.2f} words\")\n",
    "print(f\"  Average processed length: {processed_lengths.mean():.2f} words\")\n",
    "print(f\"  Average reduction: {original_lengths.mean() - processed_lengths.mean():.2f} words\")\n",
    "print(f\"  Percentage reduction: {(1 - processed_lengths.mean()/original_lengths.mean())*100:.2f}%\")\n",
    "\n",
    "# Show most common words before and after\n",
    "from collections import Counter\n",
    "\n",
    "# Get all words from original reviews\n",
    "all_original_words = []\n",
    "for review in df['review']:\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    all_original_words.extend(tokens)\n",
    "\n",
    "# Get all words from processed reviews\n",
    "all_processed_words = []\n",
    "for review in df['processed_review']:\n",
    "    tokens = word_tokenize(review)\n",
    "    all_processed_words.extend(tokens)\n",
    "\n",
    "# Count frequencies\n",
    "original_word_counts = Counter(all_original_words)\n",
    "processed_word_counts = Counter(all_processed_words)\n",
    "\n",
    "print(f\"\\nMost common words BEFORE preprocessing:\")\n",
    "for word, count in original_word_counts.most_common(10):\n",
    "    print(f\"  '{word}': {count} occurrences\")\n",
    "\n",
    "print(f\"\\nMost common words AFTER preprocessing:\")\n",
    "for word, count in processed_word_counts.most_common(10):\n",
    "    print(f\"  '{word}': {count} occurrences\")\n",
    "\n",
    "output_filename = 'imdb_reviews_processed.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nProcessed dataset saved as '{output_filename}'\")\n",
    "\n",
    "# Create a summary dataframe for the report\n",
    "summary_data = []\n",
    "for i, example in enumerate(examples):\n",
    "    result = preprocess_text(example['original_text'])\n",
    "    summary_data.append({\n",
    "        'Example': i+1,\n",
    "        'Sentiment': 'Positive' if example['sentiment'] == 1 else 'Negative',\n",
    "        'Original_Text': example['original_text'][:100] + '...' if len(example['original_text']) > 100 else example['original_text'],\n",
    "        'Lowercase': result['lowercase'][:100] + '...' if len(result['lowercase']) > 100 else result['lowercase'],\n",
    "        'No_Punctuation': result['no_punctuation'][:100] + '...' if len(result['no_punctuation']) > 100 else result['no_punctuation'],\n",
    "        'No_Numbers': result['no_numbers'][:100] + '...' if len(result['no_numbers']) > 100 else result['no_numbers'],\n",
    "        'No_Stopwords': result['no_stopwords'][:100] + '...' if len(result['no_stopwords']) > 100 else result['no_stopwords'],\n",
    "        'Lemmatized': result['lemmatized'][:100] + '...' if len(result['lemmatized']) > 100 else result['lemmatized']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary table for report (first 100 characters shown):\")\n",
    "print(summary_df.to_string())\n",
    "\n",
    "# Also save this summary\n",
    "summary_df.to_csv('preprocessing_examples_summary.csv', index=False)\n",
    "print(f\"\\nPreprocessing examples summary saved as 'preprocessing_examples_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11511d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
